{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "import params\n",
    "from utils.sequence_data import generator_for_binary_classifier, generator_to_samples_and_targets\n",
    "from utils.metrics import print_report_for_binary_classfier\n",
    "from utils.preprocessing import probs_to_binary_classes\n",
    "from utils.plot import plot_train_validation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../datasets/data_for_models/dataset_1996-01-01_2019-08-22.csv\",\n",
    "                           header=0, parse_dates=[0], index_col=0)\n",
    "\n",
    "#params for generator\n",
    "label_index = len(dataset.columns) -1\n",
    "input_shape = (params.LOOKBACK//params.STEP, dataset.shape[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_idx = math.ceil(len(dataset)*params.TRAIN_RATIO)\n",
    "val_max_idx = math.ceil(len(dataset)*(params.TRAIN_RATIO+params.VAL_RATIO))\n",
    "\n",
    "# 1 step = 1 batche of samples \n",
    "train_steps = (train_max_idx+1 -params.LOOKBACK) // params.BATCH_SIZE\n",
    "val_steps =  (val_max_idx - train_max_idx - params.LOOKBACK) // params.BATCH_SIZE\n",
    "test_steps = (len(dataset) - val_max_idx - params.LOOKBACK) // params.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init generator_for_binary_classifiers\n",
    "train_gen = generator_for_binary_classifier(dataset.to_numpy(),\n",
    "                      label_index=label_index,\n",
    "                      lookback=params.LOOKBACK,\n",
    "                      delay=params.DELAY,\n",
    "                      min_index=0,\n",
    "                      max_index=train_max_idx,\n",
    "                      shuffle=False,\n",
    "                      step=params.STEP, \n",
    "                      batch_size=params.BATCH_SIZE,\n",
    "                      interval_label=True)\n",
    "\n",
    "\n",
    "\n",
    "val_gen = generator_for_binary_classifier(dataset.to_numpy(),\n",
    "                    label_index=label_index,\n",
    "                      lookback=params.LOOKBACK,\n",
    "                      delay=params.DELAY,\n",
    "                      min_index=train_max_idx+1,\n",
    "                      max_index=val_max_idx,\n",
    "                      shuffle=False,\n",
    "                      step=params.STEP, \n",
    "                      batch_size=params.BATCH_SIZE,\n",
    "                      interval_label=True)\n",
    "\n",
    "test_gen = generator_for_binary_classifier(dataset.to_numpy(),\n",
    "                      label_index=label_index,\n",
    "                      lookback=params.LOOKBACK,\n",
    "                      delay=params.DELAY,\n",
    "                      min_index=val_max_idx+1,\n",
    "                      max_index=None,\n",
    "                      shuffle=False,\n",
    "                      step=params.STEP, \n",
    "                      batch_size=params.BATCH_SIZE,\n",
    "                      interval_label=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_model(hidden_unit, n_layer, l2_weight, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    \n",
    "    for _ in range(n_layer):\n",
    "        model.add(Dense(hidden_unit, activation='relu', kernel_regularizer=regularizers.l2(l2_weight)))\n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_avg_model_loss(history):\n",
    "    \"\"\"return the epoch and its min avg loss\"\"\"\n",
    "    loss = np.array(history.history['loss'])\n",
    "    val_loss = np.array(history.history['val_loss'])\n",
    "    avg_loss = list((loss + val_loss)/2)\n",
    "    min_idx = avg_loss.index(min(avg_loss))\n",
    "\n",
    "    return (min_idx+1), avg_loss[min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparams_search(model_func, train_steps, val_steps, train_gen, val_gen, num_epoch, hidden_units, n_layers, l2_weights):\n",
    "    \"\"\"return the best combination of params with minimal average of train and val loss\"\"\"\n",
    "    min_avg_loss=999999\n",
    "    best_params={}\n",
    "    for l2_weight in l2_weights:\n",
    "        for n_layer in n_layers:\n",
    "            for hidden_unit in hidden_units:\n",
    "                # compile \n",
    "                model = model_func(hidden_unit, n_layer, l2_weight, input_shape)\n",
    "                \n",
    "                # fit the model\n",
    "                history = model.fit_generator(train_gen,\n",
    "                                    steps_per_epoch=train_steps,\n",
    "                                    validation_data=val_gen,\n",
    "                                    validation_steps=val_steps,\n",
    "                                    epochs=num_epoch, verbose=0)\n",
    "                \n",
    "                cur_num_epoch, cur_avg_loss = min_avg_model_loss(history)\n",
    "                \n",
    "                if(cur_avg_loss < min_avg_loss):\n",
    "                    best_params['epochs'] = cur_num_epoch\n",
    "                    best_params['n_layer'] = n_layer\n",
    "                    best_params['hidden_unti'] = hidden_unit\n",
    "                    best_params['l2_weight'] = l2_weight\n",
    "                    min_avg_loss = cur_avg_loss\n",
    "                    \n",
    "    return best_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = hyperparams_search(model_func=fc_model, \n",
    "                                 train_steps=train_steps, \n",
    "                                 val_steps=val_steps, \n",
    "                                 train_gen=train_gen, \n",
    "                                 val_gen=val_gen, \n",
    "                                 num_epoch=3000,\n",
    "                                 hidden_units=[16,32,64], \n",
    "                                 n_layers=[1,2,3,4], \n",
    "                                 l2_weights=[0,0.0005,0.00075 ,0.001, 0.0015,0.01])\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "num_epoch = best_params['epochs'] \n",
    "n_layer= best_params['n_layer'] \n",
    "hidden_unit=best_params['hidden_unti']  \n",
    "l2_weight=best_params['l2_weight']\n",
    "                    \n",
    "\n",
    "model = fc_model(hidden_unit, n_layer, l2_weight, input_shape)\n",
    "\n",
    "# #fit the model\n",
    "history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=num_epoch, verbose=0)\n",
    "\n",
    "plot_train_validation_metric(history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have 2984 positive and 984 negtive cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that takes lists of hyperparameters and return the best model with certain combination of hyperparameters.\n",
    "\n",
    "def hyperparams_search(epochs, l2_weights, n_layers, hidden_units):\n",
    "    ...\n",
    "    \n",
    "    return the best combination minimal average of train and val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Make predictions for train set\n",
    "    \n",
    "X, Y = generator_to_samples_and_targets(train_gen, train_steps)    \n",
    "Y_pred = model.predict(X)\n",
    "print(collections.Counter(Y))\n",
    "print_report_for_binary_classfier(Y, probs_to_binary_classes(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make predictions from dev set\n",
    "X, Y = generator_to_samples_and_targets(val_gen, val_steps)    \n",
    "Y_pred = model.predict(X)\n",
    "print(collections.Counter(Y))\n",
    "print_report_for_binary_classfier(Y, probs_to_binary_classes(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions for test set\n",
    "X, Y = generator_to_samples_and_targets(test_gen, test_steps)    \n",
    "Y_pred = model.predict(X)\n",
    "print(collections.Counter(Y))\n",
    "print_report_for_binary_classfier(Y, probs_to_binary_classes(Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save(params.BEST_MODEL_PATH)  # creates a HDF5 file 'my_model.h5'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
